# -*- coding: utf-8 -*-
"""AMY Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFLUhEe1xMrKgRqRMjRQH2IHVPXIebNg
"""

#import numpy as np 
import pandas as pd 
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import seaborn as sns
import PIL
from typing import List


import tensorflow as tf

# EfficientNet
from tensorflow.keras.applications import EfficientNetB7, ResNet50
from tensorflow.keras.applications.efficientnet import preprocess_input

# Data Augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Model Layers
from tensorflow.keras import Model, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, BatchNormalization

# Compiling and Callbacks
from tensorflow.keras.optimizers import SGD,Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
#-----------------------------------------------------------------------------------------------------
# Competition Directory
comp_dir="/kaggle/input/ranzcr-clip-catheter-line-classification/"

# Get Training Data Labels
df_train=pd.read_csv(comp_dir+"train.csv").sample(frac=1).reset_index(drop=True)

# Get Training/Testing Data Paths
test_files = os.listdir(comp_dir+"test")      

df_test = pd.DataFrame({"StudyInstanceUID": test_files})

image_size = 256
batch_size = 16
num_epochs = 15
learn_rate = 1e-03
df_train.StudyInstanceUID += ".jpg"

# -----------------------------------------------------------------------------------------------------
base_model = ResNet50(include_top=False, 
                                    weights="imagenet", 
                                    input_shape=(image_size, image_size, 3))
#-----------------------------------------------------------------------------------------------------
label_cols=df_train.columns.tolist()
label_cols.remove("StudyInstanceUID")
label_cols.remove("PatientID")
datagen=ImageDataGenerator(rescale=1./255.)
test_datagen=ImageDataGenerator(rescale=1./255.)
# -----------------------------------------------------------------------------------------------------
# ETT CATHETER (ABNORMAL, BORDERLINE, NORMAL)
## DATFRAME FILTER
ETT_df_train = df_train[["StudyInstanceUID", "ETT - Abnormal", "ETT - Borderline", "ETT - Normal"]]
ETT_df_test = df_test[["StudyInstanceUID", "ETT - Abnormal", "ETT - Borderline", "ETT - Normal"]]
# print(ETT_df.head())

# ETT GENERATORS 
ETT_dftrain_generator=datagen.flow_from_dataframe(
    dataframe=ETT_df_train[:21000],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

ETT_dfvalid_generator=test_datagen.flow_from_dataframe(
    dataframe=ETT_df_train[21000:],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

ETT_dftest_generator=test_datagen.flow_from_dataframe(
    dataframe=ETT_df_test,
    directory=comp_dir+"test",
    x_col="StudyInstanceUID",
    batch_size=1,
    seed=42,
    shuffle=False,
    color_mode="rgb",
    class_mode=None,
    target_size=(image_size,image_size),
    interpolation="bilinear")

## CONSISTENT PARAMETERS                                      
inp = Input(shape = (image_size,image_size,3))
x = base_model(inp)
x = Flatten()(x)    

## BUILDING THE MODEL (Keras’s Function API )                                      

# CONV LAYER 1
x = Conv2D(32, (7, 7),  activation='relu', padding = 'same', name='conv1')(inp)  # NOTE: very 1st layer input

# CONV LAYER 2
x = Conv2D(64, (7, 7), activation='relu',  padding = 'same', name='conv2')(x)
x = MaxPooling2D(pool_size = (2, 2))(x)
x = Dropout(0.25)(x)

# CONV LAYER 3
x = Conv2D(128, (5, 5), activation='relu',  padding = 'same', name='conv3')(x)

# CONV LAYER 4
x = Conv2D(256, (5, 5), activation='relu',  padding = 'same', name='conv4')(x)
x = MaxPooling2D(pool_size = (2, 2))(x)
x = Dropout(0.25)(x)

# CONV LAYER 5
x = Conv2D(512, (3, 3), activation='relu',  padding = 'same', name='conv5')(x)

# CONV LAYER 6
x = Conv2D(1024, (3, 3), activation='relu',  padding = 'same', name='conv6')(x)
x = MaxPooling2D(pool_size = (2, 2))(x)
x = Dropout(0.25)(x)

# FINALE
x = Flatten()(x)
x = Dense(512)(x)
x = Activation('relu')(x)
x = Dropout(0.5)(x)


# “binary_crossentropy” as loss function and “sigmoid” as the final layer activation
output1 = Dense(1, activation = 'sigmoid')(x)
output2 = Dense(1, activation = 'sigmoid')(x)
output3 = Dense(1, activation = 'sigmoid')(x)
model = Model(inp,[output1,output2,output3])
model.compile(optimizers.rmsprop(lr = 0.0001, decay = 1e-6),
loss = ["binary_crossentropy","binary_crossentropy","binary_crossentropy"],metrics = ["accuracy"])             
                                           

## GENERATOR WRAPPER (3 outputs)
def generator_wrapper(generator):
    for batch_x,batch_y in generator:
        yield (batch_x,[batch_y[:,i] for i in range(3)])

                                           
## FITTING AND TRAINING THE MODEL                                        
ETT_STEP_SIZE_TRAIN=ETT_dftrain_generator.n//ETT_dftrain_generator.batch_size
ETT_STEP_SIZE_VALID=ETT_dfvalid_generator.n//ETT_dfvalid_generator.batch_size
ETT_STEP_SIZE_TEST=ETT_dftest_generator.n//ETT_dftest_generator.batch_size
model.fit_generator(generator=train_generator,
                    steps_per_epoch=ETT_STEP_SIZE_TRAIN,
                    validation_data=ETT_dfvalid_generator,
                    validation_steps=ETT_STEP_SIZE_VALID,
                    epochs=10
)
                                           
                                           
## PREDICITING THE OUTPUT
                                                                                 
ETT_dftest_generator.reset()
pred=model.predict_generator(ETT_dftest_generator,
steps=ETT_STEP_SIZE_TEST,verbose=1)
# -----------------------------------------------------------------------------------------------------
# CVC CATHETER (ABNORMAL, BORDERLINE, NORMAL)
## DATFRAME FILTER
CVC_df_train = df_train[["StudyInstanceUID", "CVC - Abnormal", "CVC - Borderline", "CVC - Normal"]]
CVC_df_test = df_test[["StudyInstanceUID", "CVC - Abnormal", "CVC - Borderline", "CVC - Normal"]]
# print(CVC_df.head())

# CVC GENERATORS 
CVC_dftrain_generator=datagen.flow_from_dataframe(
    dataframe=CVC_df_train[:21000],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

CVC_dfvalid_generator=test_datagen.flow_from_dataframe(
    dataframe=CVC_df_train[21000:],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

CVC_dftest_generator=test_datagen.flow_from_dataframe(
    dataframe=CVC_df_test,
    directory=comp_dir+"test",
    x_col="StudyInstanceUID",
    batch_size=1,
    seed=42,
    shuffle=False,
    color_mode="rgb",
    class_mode=None,
    target_size=(image_size,image_size),
    interpolation="bilinear")

## CONSISTENT PARAMETERS                                      
inp = Input(shape = (image_size,image_size,3))
x = base_model(inp)
x = Flatten()(x)    

## BUILDING THE MODEL (Keras’s Function API )             

#CONVOLUTION LAYERS -> would go here


# “binary_crossentropy” as loss function and “sigmoid” as the final layer activation
output4 = Dense(1, activation = 'sigmoid')(x)
output5 = Dense(1, activation = 'sigmoid')(x)
output6 = Dense(1, activation = 'sigmoid')(x)
model = Model(inp,[output4,output5,output6])
model.compile(optimizers.rmsprop(lr = 0.0001, decay = 1e-6),
loss = ["binary_crossentropy","binary_crossentropy","binary_crossentropy"],metrics = ["accuracy"])             
                                           

## GENERATOR WRAPPER (3 outputs)
def generator_wrapper(generator):
    for batch_x,batch_y in generator:
        yield (batch_x,[batch_y[:,i] for i in range(3)])

                                           
## FITTING AND TRAINING THE MODEL                                        
CVC_STEP_SIZE_TRAIN=CVC_dftrain_generator.n//CVC_dftrain_generator.batch_size
CVC_STEP_SIZE_VALID=CVC_dfvalid_generator.n//CVC_dfvalid_generator.batch_size
CVC_STEP_SIZE_TEST=CVC_dftest_generator.n//CVC_dftest_generator.batch_size
model.fit_generator(generator=train_generator,
                    steps_per_epoch=CVC_STEP_SIZE_TRAIN,
                    validation_data=CVC_dfvalid_generator,
                    validation_steps=CVC_STEP_SIZE_VALID,
                    epochs=10
)
                                           
                                           
## PREDICITING THE OUTPUT
                                                                                 
CVC_dftest_generator.reset()
pred=model.predict_generator(CVC_dftest_generator,
steps=CVC_STEP_SIZE_TEST,verbose=1)
# -----------------------------------------------------------------------------------------------------
# NGT CATHETER (ABNORMAL, BORDERLINE, NORMAL)
## DATFRAME FILTER
NGT_df_train = df_train[["StudyInstanceUID", "NGT - Abnormal", "NGT - Borderline", "NGT - Incompletely Imaged", "NGT - Normal"]]
NGT_df_test = df_test[["StudyInstanceUID", "NGT - Abnormal", "NGT - Borderline", "NGT - Incompletely Imaged", "NGT - Normal"]]
# print(NGT_df.head())

# NGT GENERATORS 
NGT_dftrain_generator=datagen.flow_from_dataframe(
    dataframe=NGT_df_train[:21000],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

NGT_dfvalid_generator=test_datagen.flow_from_dataframe(
    dataframe=NGT_df_train[21000:],
    directory=comp_dir+"train",
    x_col="StudyInstanceUID",
    y_col=label_cols,
    batch_size=batch_size,
    seed=42,
    shuffle=True,
    color_mode="rgb",
    class_mode="raw",
    target_size=(image_size,image_size),
    interpolation="bilinear")

NGT_dftest_generator=test_datagen.flow_from_dataframe(
    dataframe=NGT_df_test,
    directory=comp_dir+"test",
    x_col="StudyInstanceUID",
    batch_size=1,
    seed=42,
    shuffle=False,
    color_mode="rgb",
    class_mode=None,
    target_size=(image_size,image_size),
    interpolation="bilinear")

## CONSISTENT PARAMETERS                                      
inp = Input(shape = (image_size,image_size,3))
x = base_model(inp)
x = Flatten()(x)    

## BUILDING THE MODEL (Keras’s Function API )             

#CONVOLUTION LAYERS -> would go here

# FINALE
x = Flatten()(x)
x = Dense(512)(x)
x = Activation('relu')(x)
x = Dropout(0.5)(x)


# “binary_crossentropy” as loss function and “sigmoid” as the final layer activation
output7 = Dense(1, activation = 'sigmoid')(x)
output8 = Dense(1, activation = 'sigmoid')(x)
output9 = Dense(1, activation = 'sigmoid')(x)
output10 = Dense(1, activation = 'sigmoid')(x)
model = Model(inp,[output7,output8,output9,output10])
model.compile(optimizers.rmsprop(lr = 0.0001, decay = 1e-6),
loss = ["binary_crossentropy","binary_crossentropy","binary_crossentropy","binary_crossentropy"],metrics = ["accuracy"])             
                                           

## GENERATOR WRAPPER (3 outputs)
def generator_wrapper(generator):
    for batch_x,batch_y in generator:
        yield (batch_x,[batch_y[:,i] for i in range(3)])

                                           
## FITTING AND TRAINING THE MODEL                                        
NGT_STEP_SIZE_TRAIN=NGT_dftrain_generator.n//NGT_dftrain_generator.batch_size
NGT_STEP_SIZE_VALID=NGT_dfvalid_generator.n//NGT_dfvalid_generator.batch_size
NGT_STEP_SIZE_TEST=NGT_dftest_generator.n//NGT_dftest_generator.batch_size
model.fit_generator(generator=NGT_dftrain_generator,
                    steps_per_epoch=NGT_STEP_SIZE_TRAIN,
                    validation_data=NGT_dfvalid_generator,
                    validation_steps=NGT_STEP_SIZE_VALID,
                    epochs=10
)
                                           
                                           
## PREDICITING THE OUTPUT
                                                                                 
NGT_dftest_generator.reset()
pred=model.predict_generator(NGT_dftest_generator,
steps=NGT_STEP_SIZE_TEST,verbose=1)
# -----------------------------------------------------------------------------------------------------
#SWAN GANZ CATHETER (Present)











                                           
 
# ----------------------------------------------------------------------------------------------------- 
#FINAL OUTPUT -> putting together all four predictions      
## NEED TO DECIDE ON WHAT OUTPUT FORMAT WE WANT (2 OPTIONS OUTLINED, COULD CHANGE IF NECESSARY)                                          

predictions = pred_bool.astype(int)
columns=["ETT - Abnormal", "ETT - Borderline", "ETT - Normal"]
#columns should be the same order of y_col
results=pd.DataFrame(predictions, columns=columns)
results["StudyInstanceUID"]=test_generator.StudyInstanceUID
ordered_cols=["StudyInstanceUID"]+columns
results=results[ordered_cols]#To get the same column order
results.to_csv("results.csv",index=False)
